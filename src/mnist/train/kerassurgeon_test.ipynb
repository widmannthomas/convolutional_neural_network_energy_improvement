{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, AveragePooling2D\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Activation, Flatten\n",
    "import tensorflow_model_optimization as tfmot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (5,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is how a one-hot encoded 5 looks like\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "with np.load('mnist.npz') as f:\n",
    "    x_train, y_train = f['x_train'], f['y_train']\n",
    "\n",
    "x_train = x_train/255 # Normalize pixel values to [0,1] and flatten each image to 1D.\n",
    "y_train = keras.utils.to_categorical(y_train) # One-hot encoding\n",
    "\n",
    "print('This is how a one-hot encoded 5 looks like')\n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_base_model():\n",
    "    raw_model = Sequential()\n",
    "    raw_model.add(Conv2D(input_shape=(28, 28, 1), kernel_size=(16, 16), filters=32, strides=(4, 4), activation='relu', name=\"conv1\"))\n",
    "    raw_model.add(Conv2D(kernel_size=(3, 3), filters=64, strides=(1, 1), activation='relu', name=\"conv2\"))\n",
    "    raw_model.add(Flatten())\n",
    "    raw_model.add(Dense(10, activation='softmax'))\n",
    "    return raw_model\n",
    "\n",
    "def create_lenet():\n",
    "    lenet_5_model = Sequential([\n",
    "        Conv2D(filters=6, kernel_size=5, strides=1,  activation='relu', input_shape=(28, 28, 1), padding='same', name=\"conv1\"), #C1\n",
    "        AveragePooling2D(), #S2\n",
    "        Conv2D(filters=16, kernel_size=5, strides=1, activation='relu', padding='valid', name=\"conv2\"), #C3\n",
    "        AveragePooling2D(), #S4\n",
    "        Flatten(), #Flatten\n",
    "        Dense(120, activation='tanh'), #C5\n",
    "        Dense(84, activation='tanh'), #F6\n",
    "        Dense(10, activation='softmax') #Output layer\n",
    "    ])\n",
    "    return lenet_5_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1 (Conv2D)               (None, 28, 28, 6)         156       \n",
      "_________________________________________________________________\n",
      "average_pooling2d (AveragePo (None, 14, 14, 6)         0         \n",
      "_________________________________________________________________\n",
      "conv2 (Conv2D)               (None, 10, 10, 16)        2416      \n",
      "_________________________________________________________________\n",
      "average_pooling2d_1 (Average (None, 5, 5, 16)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 120)               48120     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 84)                10164     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                850       \n",
      "=================================================================\n",
      "Total params: 61,706\n",
      "Trainable params: 61,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-30 16:54:43.196214: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "#model = create_base_model()\n",
    "model = create_lenet()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-30 16:54:45.645369: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 8s 63ms/step - loss: 0.9249 - accuracy: 0.7349\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Accuracy')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Fit the model:\n",
    "model.compile(optimizer='RMSprop',\n",
    "          loss='categorical_crossentropy',\n",
    "          metrics=['accuracy'])\n",
    "history = model.fit(x_train.reshape(-1,28,28,1), y_train, batch_size=512, epochs=1)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x_train.reshape(-1,28,28,1), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model prediction: 9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fe2b0807c10>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# classify image:\n",
    "k = 4\n",
    "image = x_train[k]\n",
    "y_predict = np.argmax(model.predict(image[None, :,:, None]))\n",
    "\n",
    "print(\"Model prediction:\", y_predict)\n",
    "plt.imshow(image.reshape(28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"mnist.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prune the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
    "\n",
    "# Compute end step to finish pruning after 2 epochs.\n",
    "batch_size = 512\n",
    "epochs = 5\n",
    "validation_split = 0.1 # 10% of training set will be used for validation set. \n",
    "\n",
    "num_images = x_train.shape[0] * (1 - validation_split)\n",
    "end_step = np.ceil(num_images / batch_size).astype(np.int32) * epochs\n",
    "\n",
    "print(\"end_step:\", end_step)\n",
    "\n",
    "# Define model for pruning.\n",
    "pruning_params = {\n",
    "      'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.5,\n",
    "                                                               final_sparsity=0.20,\n",
    "                                                               begin_step=0,\n",
    "                                                               end_step=end_step)\n",
    "}\n",
    "\n",
    "raw_model = create_base_model()\n",
    "#raw_model.load_weights(pretrained_weights)\n",
    "\n",
    "model_pruned = prune_low_magnitude(raw_model, **pruning_params)\n",
    "\n",
    "# `prune_low_magnitude` requires a recompile.\n",
    "model_pruned.compile(loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#model_for_pruning.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "  tfmot.sparsity.keras.UpdatePruningStep(),\n",
    "  #tfmot.sparsity.keras.PruningSummaries(log_dir=\"log\")\n",
    "]\n",
    "\n",
    "model_pruned.fit(x_train.reshape(-1,28,28,1), y_train,\n",
    "                  batch_size=batch_size, epochs=epochs, validation_split=validation_split,\n",
    "                  callbacks=callbacks)\n",
    "\n",
    "\n",
    "#%tensorboard --logdir={\"log\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pruned_export = tfmot.sparsity.keras.strip_pruning(model_pruned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_weights = 0\n",
    "total_weights_0 = 0\n",
    "\n",
    "for idx, layer in enumerate(model_pruned.layers):\n",
    "    weights_layer = 0\n",
    "    weights_0 = 0\n",
    "    \n",
    "    if len(layer.weights) == 0:\n",
    "        continue\n",
    "\n",
    "    for weight in np.nditer(layer.weights):\n",
    "        if type(weight) != tuple:\n",
    "            continue\n",
    "\n",
    "        weights_layer += 1\n",
    "        if weight[0] == 0:\n",
    "            weights_0 += 1\n",
    "\n",
    "    print(\"Layer: \", idx, \"Zero weights:\", weights_0, \"/\", weights_layer)\n",
    "    total_weights += weights_layer\n",
    "    total_weights_0 += weights_0\n",
    "\n",
    "print(\"Total 0-weights:\", total_weights_0, \"/\", total_weights, \" => \", (total_weights_0 / total_weights) * 100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def representative_dataset_gen():\n",
    "  for i in range(100):\n",
    "    yield [x_train[i].reshape(-1,28,28,1).astype(np.float32)]\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model_pruned_export)\n",
    "#converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "\n",
    "# NOTE: The current version of TensorFlow appears to break the model when using optimizations\n",
    "#    You can try uncommenting the following if you would like to generate a smaller size .tflite model\n",
    "#converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_dataset_gen\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "tflite_model = converter.convert()\n",
    "open(\"model.tflite\", \"wb\").write(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the tflite model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter = tf.lite.Interpreter(model_path=\"model.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "input_quantization_parameters = input_details[0]['quantization_parameters']\n",
    "output_quantization_parameters = output_details[0]['quantization_parameters']\n",
    "\n",
    "print(\"== Input details ==\")\n",
    "print(\"name:\", interpreter.get_input_details()[0]['name'])\n",
    "print(\"shape:\", interpreter.get_input_details()[0]['shape'])\n",
    "print(\"type:\", interpreter.get_input_details()[0]['dtype'])\n",
    "print(\"quantization scale:\", input_quantization_parameters['scales'])\n",
    "print(\"quantization zero point:\", input_quantization_parameters['zero_points'])\n",
    "\n",
    "print(\"\\n== Output details ==\")\n",
    "print(\"name:\", interpreter.get_output_details()[0]['name'])\n",
    "print(\"shape:\", interpreter.get_output_details()[0]['shape'])\n",
    "print(\"type:\", interpreter.get_output_details()[0]['dtype'])\n",
    "print(\"quantization scale:\", output_quantization_parameters['scales'])\n",
    "print(\"quantization zero point:\", output_quantization_parameters['zero_points'])\n",
    "\n",
    "print(\"\\nDUMP INPUT\")\n",
    "print(interpreter.get_input_details()[0])\n",
    "print(\"\\nDUMP OUTPUT\")\n",
    "print(interpreter.get_output_details()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(interpreter, input_data):\n",
    "    interpreter.set_tensor(input_details[0][\"index\"], input_data)\n",
    "    interpreter.invoke()\n",
    "    output_data = interpreter.get_tensor(output_details[0][\"index\"])\n",
    "    return output_data\n",
    "\n",
    "\n",
    "def measure_accuracy(interpreter, samples = len(x_train)):\n",
    "    output_data = []\n",
    "\n",
    "    for idx, sample in enumerate(x_train[:samples]):\n",
    "        input_data = np.array(sample, dtype=np.float32)\n",
    "        input_data = np.expand_dims(input_data, axis=0)\n",
    "        input_data = np.expand_dims(input_data, axis=3)\n",
    "        print(np.shape(input_data))\n",
    "        output_data.append(predict(interpreter, input_data))\n",
    "\n",
    "    a = [np.argmax(y, axis=None, out=None) for y in output_data]\n",
    "    b = [np.argmax(y, axis=None, out=None) for y in y_train[:samples]]\n",
    "\n",
    "    accuracy = (np.array(a) == np.array(b)).mean()\n",
    "    print(\"TFLite Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measure_accuracy(interpreter, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save .TFLite as C Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import binascii\n",
    "\n",
    "def convert_to_c_array(bytes) -> str:\n",
    "  hexstr = binascii.hexlify(bytes).decode(\"UTF-8\")\n",
    "  hexstr = hexstr.upper()\n",
    "  array = [\"0x\" + hexstr[i:i + 2] for i in range(0, len(hexstr), 2)]\n",
    "  array = [array[i:i+10] for i in range(0, len(array), 10)]\n",
    "  return \",\\n  \".join([\", \".join(e) for e in array])\n",
    "\n",
    "tflite_binary = open(\"model.tflite\", 'rb').read()\n",
    "ascii_bytes = convert_to_c_array(tflite_binary)\n",
    "c_file = \"alignas(8) const unsigned char g_model[] = {\\n  \" + ascii_bytes + \"\\n};\\nconst unsigned int g_model_len = \" + str(len(tflite_binary)) + \";\"\n",
    "open(\"model.h\", \"w\").write(c_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pruning with kerassurgeon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kerassurgeon\n",
    "from kerassurgeon import operations\n",
    "from kerassurgeon import identify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1\n",
      "average_pooling2d\n",
      "conv2\n",
      "average_pooling2d_1\n",
      "flatten\n",
      "dense\n",
      "dense_1\n",
      "dense_2\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    print(layer.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer2 = model.get_layer(\"conv1\")\n",
    "print(np.shape(layer2.get_weights()[0]))\n",
    "print(np.shape(layer2.get_weights()[0][:,:,0,0]))\n",
    "\n",
    "apoz = kerassurgeon.identify.get_apoz(model, layer2, x_train)\n",
    "print(\"Apoz:\", apoz)\n",
    "high_apoz_channels = kerassurgeon.identify.high_apoz(apoz, \"both\")\n",
    "print(\"High apoz:\", high_apoz_channels)\n",
    "\n",
    "weights = layer2.get_weights()\n",
    "num_of_filters = layer2.filters\n",
    "\n",
    "for i in range(0, num_of_filters):\n",
    "    filter = weights[0][:,:,0,i]\n",
    "    plt.title(\"APOZ:\" + str(int(apoz[i] * 100)) + \"%\")\n",
    "    plt.suptitle(\"High apoz: \" + str(i in high_apoz_channels))\n",
    "    plt.imshow(filter, cmap=\"gray\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6,)\n"
     ]
    }
   ],
   "source": [
    "layer = model.get_layer(\"conv1\")\n",
    "apoz = kerassurgeon.identify.get_apoz(model, layer, x_train)\n",
    "print(np.shape(apoz))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "high_apoz_channels = kerassurgeon.identify.high_apoz(apoz, \"both\")\n",
    "high_apoz_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting 1/6 channels from layer: conv1\n"
     ]
    }
   ],
   "source": [
    "model_pruned_export = kerassurgeon.operations.delete_channels(model, layer, high_apoz_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1_input (InputLayer)     [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv2D)               (None, 28, 28, 5)         130       \n",
      "_________________________________________________________________\n",
      "average_pooling2d (AveragePo multiple                  0         \n",
      "_________________________________________________________________\n",
      "conv2 (Conv2D)               (None, 10, 10, 16)        2016      \n",
      "_________________________________________________________________\n",
      "average_pooling2d_1 (Average (None, 5, 5, 16)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 120)               48120     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 84)                10164     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                850       \n",
      "=================================================================\n",
      "Total params: 61,280\n",
      "Trainable params: 61,280\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_pruned_export.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model:\n",
    "model_pruned_export.compile(optimizer='RMSprop',\n",
    "          loss='categorical_crossentropy',\n",
    "          metrics=['accuracy'])\n",
    "history = model.fit(x_train.reshape(-1,28,28,1), y_train, batch_size=512, epochs=5)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pruned_export.evaluate(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kerassurgeon import utils as k_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_utils.find_nodes_in_model(model, model.get_layer(\"conv2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Iterate thru all the layers of the model\n",
    "for layer in model_pruned_export.layers:\n",
    "    if 'conv2' in layer.name:\n",
    "        weights, bias= layer.get_weights()\n",
    "        print(layer.name, filters.shape)\n",
    "        \n",
    "        #normalize filter values between  0 and 1 for visualization\n",
    "        f_min, f_max = weights.min(), weights.max()\n",
    "        filters = (weights - f_min) / (f_max - f_min)  \n",
    "        print(filters.shape[3])\n",
    "        filter_cnt=1\n",
    "        \n",
    "        #plotting all the filters\n",
    "        for i in range(filters.shape[3]):\n",
    "            #get the filters\n",
    "            filt=filters[:,:,:, i]\n",
    "            #plotting each of the channel, color image RGB channels\n",
    "            for j in range(filters.shape[0]):\n",
    "                ax= plt.subplot(filters.shape[3], filters.shape[0], filter_cnt  )\n",
    "                ax.set_xticks([])\n",
    "                ax.set_yticks([])\n",
    "                plt.imshow(filt[:,:, j])\n",
    "                filter_cnt+=1\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0d1cd94cfed04ad9427ef807a2ad11acb0c84f064da073693db7977f8eb74254"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('test': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
